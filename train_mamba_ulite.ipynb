{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1a3eec",
   "metadata": {},
   "source": [
    "# MambaU-Lite Training for Image Segmentation\n",
    "\n",
    "This notebook trains the MambaU-Lite model for a segmentation task using the following data structure:\n",
    "- train/images (.jpg)\n",
    "- train/masks (.png binary 0-255)\n",
    "- val_images (.jpg)\n",
    "- val_masks (.png binary 0-255)\n",
    "- test/images (test data without masks)\n",
    "\n",
    "After training, it predicts on test data and saves the results to a new folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40070e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Import the model and metrics\n",
    "from models.mamba_ulite import ULite\n",
    "from metric import dice_score, iou_score, DiceLoss, dice_tversky_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a1da6",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "First, we'll create a dataset class to load images and masks from the specified directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fdbf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, img_paths, mask_paths=None, transform=None, test_mode=False):\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "        self.test_mode = test_mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Resize to model input size (adjust based on your requirements)\n",
    "        image = image.resize((256, 256))\n",
    "        \n",
    "        # Convert to tensor and normalize\n",
    "        image = np.array(image).astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image.transpose(2, 0, 1))\n",
    "        \n",
    "        if self.test_mode:\n",
    "            return image, os.path.basename(img_path)\n",
    "        \n",
    "        # Load mask\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "        mask = mask.resize((256, 256))\n",
    "        \n",
    "        # Convert mask to binary (0 or 1)\n",
    "        mask = np.array(mask)\n",
    "        mask = (mask > 128).astype(np.float32)  # Convert from 0-255 to 0-1\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        if self.transform:\n",
    "            # Apply data augmentation (you can expand this)\n",
    "            if np.random.random() > 0.5:\n",
    "                image = torch.flip(image, dims=[2])  # Horizontal flip\n",
    "                mask = torch.flip(mask, dims=[2])\n",
    "                \n",
    "            if np.random.random() > 0.5:\n",
    "                image = torch.flip(image, dims=[1])  # Vertical flip\n",
    "                mask = torch.flip(mask, dims=[1])\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data paths\n",
    "data_root = input(\"Enter path to data directory: \")  # User will input the data root directory\n",
    "\n",
    "# Training data\n",
    "train_img_paths = sorted(glob(os.path.join(data_root, 'train/images/*.jpg')))\n",
    "train_mask_paths = sorted(glob(os.path.join(data_root, 'train/masks/*.png')))\n",
    "\n",
    "# Validation data\n",
    "val_img_paths = sorted(glob(os.path.join(data_root, 'val_images/*.jpg')))\n",
    "val_mask_paths = sorted(glob(os.path.join(data_root, 'val_masks/*.png')))\n",
    "\n",
    "# Test data\n",
    "test_img_paths = sorted(glob(os.path.join(data_root, 'test/images/*.jpg')))\n",
    "\n",
    "print(f\"Train images: {len(train_img_paths)}\")\n",
    "print(f\"Train masks: {len(train_mask_paths)}\")\n",
    "print(f\"Validation images: {len(val_img_paths)}\")\n",
    "print(f\"Validation masks: {len(val_mask_paths)}\")\n",
    "print(f\"Test images: {len(test_img_paths)}\")\n",
    "\n",
    "# Verify paths match for training and validation\n",
    "assert len(train_img_paths) == len(train_mask_paths), \"Number of training images and masks don't match\"\n",
    "assert len(val_img_paths) == len(val_mask_paths), \"Number of validation images and masks don't match\"\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SegmentationDataset(train_img_paths, train_mask_paths, transform=True)\n",
    "val_dataset = SegmentationDataset(val_img_paths, val_mask_paths)\n",
    "test_dataset = SegmentationDataset(test_img_paths, test_mode=True)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 4  # Adjust based on your GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=4)\n",
    "\n",
    "# Show a sample from the training dataset\n",
    "sample_idx = np.random.randint(0, len(train_dataset))\n",
    "sample_img, sample_mask = train_dataset[sample_idx]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_img.permute(1, 2, 0))\n",
    "plt.title(\"Sample Image\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(sample_mask.squeeze(), cmap='gray')\n",
    "plt.title(\"Sample Mask\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa977c",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Now we'll set up the model for training using PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a27a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = ULite()\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def _step(self, batch):\n",
    "        images, masks = batch\n",
    "        # The model now handles tensor format conversion internally\n",
    "        # No need to manually permute here\n",
    "        outputs = self.model(images)\n",
    "        loss = dice_tversky_loss(outputs, masks)\n",
    "        dice = dice_score(outputs, masks)\n",
    "        iou = iou_score(outputs, masks)\n",
    "        return loss, dice, iou\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, dice, iou = self._step(batch)\n",
    "        metrics = {\"loss\": loss, \"train_dice\": dice, \"train_iou\": iou}\n",
    "        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, dice, iou = self._step(batch)\n",
    "        metrics = {\"val_loss\": loss, \"val_dice\": dice, \"val_iou\": iou}\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "        return metrics\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, dice, iou = self._step(batch)\n",
    "        metrics = {\"test_loss\": loss, \"test_dice\": dice, \"test_iou\": iou}\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "        return metrics\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"max\", factor=0.5, patience=5, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"val_dice\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b76f1",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now we'll train the model using PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bde692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model\n",
    "model = SegmentationModel(learning_rate=1e-3)\n",
    "\n",
    "# Set up callbacks\n",
    "checkpoint_dir = os.path.join(os.getcwd(), 'checkpoints')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=checkpoint_dir,\n",
    "    filename='mamba_ulite-{epoch:02d}-{val_dice:.4f}',\n",
    "    monitor='val_dice',\n",
    "    mode='max',\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_dice',\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Set up trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    precision=16,  # Use mixed precision for faster training\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# Print best model path\n",
    "print(f\"Best model path: {checkpoint_callback.best_model_path}\")\n",
    "print(f\"Best validation dice score: {checkpoint_callback.best_model_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868818f3",
   "metadata": {},
   "source": [
    "## Prediction on Test Data\n",
    "\n",
    "Now we'll predict on the test data and save the results to a new folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6550169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = SegmentationModel.load_from_checkpoint(best_model_path)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = os.path.join(os.getcwd(), 'predictions')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Predict on test data\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        images, filenames = batch\n",
    "        \n",
    "        # No need to permute - model handles format conversion\n",
    "        \n",
    "        # Move to device\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Predict\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Convert to binary mask\n",
    "        predictions = torch.sigmoid(outputs) > 0.5\n",
    "        \n",
    "        # Save predictions\n",
    "        for i, filename in enumerate(filenames):\n",
    "            # Convert prediction to image\n",
    "            pred_mask = predictions[i].cpu().numpy().squeeze().astype(np.uint8) * 255\n",
    "            \n",
    "            # Save prediction\n",
    "            output_path = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_pred.png\")\n",
    "            cv2.imwrite(output_path, pred_mask)\n",
    "\n",
    "print(f\"Predictions saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f297d275",
   "metadata": {},
   "source": [
    "## Visualize Some Predictions\n",
    "\n",
    "Let's visualize a few test predictions alongside the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbdb841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a few test images and their predictions\n",
    "test_samples = min(5, len(test_img_paths))\n",
    "plt.figure(figsize=(15, test_samples*5))\n",
    "\n",
    "for i in range(test_samples):\n",
    "    # Load original image\n",
    "    img_path = test_img_paths[i]\n",
    "    img_name = os.path.basename(img_path)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Load prediction\n",
    "    pred_path = os.path.join(output_dir, f\"{os.path.splitext(img_name)[0]}_pred.png\")\n",
    "    pred = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Display\n",
    "    plt.subplot(test_samples, 2, i*2+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Original Image: {img_name}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(test_samples, 2, i*2+2)\n",
    "    plt.imshow(pred, cmap='gray')\n",
    "    plt.title(f\"Prediction\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d9af0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "1. Set up a dataset to load images and masks from the specified directory structure\n",
    "2. Created a PyTorch Lightning module for the MambaU-Lite model\n",
    "3. Trained the model on the training data and validated it\n",
    "4. Predicted on the test data and saved the results to a new folder\n",
    "5. Visualized some test predictions\n",
    "\n",
    "The predictions are saved in the `predictions` folder with the naming convention `{original_filename}_pred.png`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
